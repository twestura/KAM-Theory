\documentclass[twoside,letterpaper,10pt]{article}

\usepackage{mystyle}
\newcommand{\sgrad}{\nabla_{\sigma}}
% TODO make the background of theorems darker (or at least make sure that the
% printer does).
\usepackage{wrapfig}
\input{mathmacros.tex}

\numberwithin{equation}{section}

\newcommand{\T}{\mathbb{T}}

\usepackage{fancyhdr}
\setlength{\headheight}{14.0pt}
\setlength{\headsep}{0.2in}
\renewcommand{\headrulewidth}{0pt}
\cfoot{}
\pagestyle{fancy}

\renewcommand{\sectionmark}[1]{ \markright{Section \thesection. #1}{} }

\fancyhf{}
\fancyhead[LE]{\textsc{\thepage \qquad \nouppercase{\rightmark}} }
\fancyhead[RO]{\textsc{Travis Westura \qquad \thepage}}

\fancypagestyle{plain}{ %
  \fancyhf{}
  \renewcommand{\headrulewidth}{0pt}
  \renewcommand{\footrulewidth}{0pt}
}

\title{Kolmogorov's Theorem}
\author{Travis Westura}
\date{\today}

\begin{document}

\maketitle

% I know that I'm not supposed to use a citation in an abstract, but Hubbard
% did so in his paper, so I guess it is fair game.
\begin{abstract}
  This paper gives a proof of the Kolmogorov Theorem on the conservation of
  invariant tori.
  We follow the approach given by Hubbard and Ilyashenko in \cite{hi02}.
  Their proof is similar to the one given by Bennettin, Galgani, Giorgilli, and
  Strelcyn in \cite{bggs84}, which itself resembles Kolmogorov's original
  argument.
  % TODO write citations and confirm correctness of the last claim by rereading
  % the other paper's abstract.
  % Also check the spelling of their names and add them to the dictionary.
  % Do by 4/27.
\end{abstract}


\section{Introduction}
\label{sec:introduction}

% TODO Write up historical information in the next paragraph.

Arnold recalls in \cite{a97} how Kolmogorov was influenced by \cite{f64} as a
child.
% TODO fix citations, just here right now to get a compilation

But before moving on, let's take a look at our main goal:
\begin{thm}[The Kolmogorov Theorem]
  \label{thm:KAM}
  \KAM{}
\end{thm}
We will return to this statement after building up some intuition for it.

\section{A Motivating Example}
\label{sec:motivating-example}

To start understanding the theorem, let's consider an example to which we would
like to apply it.
While staring up at the night sky, one might be driven to wonder, ``Why do the
planets orbit around the sun and not just crash into the sun or fly off by
themselves in their own directions?''
Kolmogorov's Theorem gives us a method to attempt to answer this question.

Let's start by discussing a simplified model of our solar system.
In this model we will assume that planets have zero mass.
This assumption is reasonable, as the masses of the planets are extremely small
compared to that of the sun.
Thus we may, at least for the moment, consider these masses to be negligible.

A system with $n$ bodies, each with a mass $m_i$ and a position $\bp{x}_i$
satisfies Netwon's second law: $\bp{F} = m \bp{a}$.
For each $i$, we have
\begin{equation*}
  m_i \bdd{x} = \sum_{j \neq i} \mathrm{G} \nobreak\hspace{.125em plus
    .14286em} m_i m_j \frac{\bp{x}_j - \bp{x}_i}{|\bp{x}_j - \bp{x}_i|^3}.
\end{equation*}
Here, $\mathrm{G} \approx 6.62 \cdot 10^{-11} m^3 / (kg\, s^2)$ is the universal
gravitational constant.
On the left hand side we have each planet's mass and its acceleration.
On the right hand side we have the force that acts on this mass.
Note that the force is inversely proportional to the square of the distances
between the forces.
The top of the fraction contains the vector $\bp{x}_j - \bp{x}_i$ that has
length $|\bp{x}_j - \bp{x}_i|$.
To cancel out this length, we divide by $|\bp{x}_j - \bp{x}_i|^3$.
This gives us the desired inverse proportionality to the square of the distance.

In our model of the solar system we take the sun to be given by the index $0$
and the planets given by indices from $1$ to $8$ or $9$, depending on the
reader's opinions of Pluto.
Now we will examine what happens as the masses of the planets, the $m_j$ for $j
= 1, \ldots, n$, tend to zero.

Rewriting the previous equations, we have
\begin{align*}
  \bdd{x}_0 &= \mathrm{G} \sum_{j = 1}^n m_j \frac{\bp{x}_j -
              \bp{x}_0}{|\bp{x}_j = \bp{x}_i|^3},\\
  \bdd{x}_i &= \mathrm{G} m_0 \frac{\bp{x}_0 - \bp{x}_i}{|\bp{x}_0 -
              \bp{x}_i|^3} + \mathrm{G} \nobreak\hspace{.125em plus
              .14286em}
              \sum_{\mathclap{\substack{j=1,\ldots,n\\j\neq i}}}
              m_j \frac{\bp{x}_j - \bp{x}_i}{|\bp{x}_j - \bp{x}_i|^3}.
\end{align*}
Now we keep the mass of the sun $m_0$ constant but let the masses of the planets
go to zero.
These equations then become
\begin{align*}
  \bdd{x}_0 &= 0,\\
  \bdd{x}_j &= \mathrm{G} \nobreak\hspace{.125em plus .14286em} \frac{\bp{x}_0 -
              \bp{x}_i}{|\bp{x}_0 - \bp{x}_i|^3}.
\end{align*}
We can even simplify these equations a bit further.
Since $\bdd{x}_0 = 0$, the body with mass $m_0$ travels in a straight line with
constant speed.
Thus we can work in a heliocentric system of coordinates with the sun at the
center of our solar system with position $\bp{x}_0 = 0$.
We then have
\begin{align*}
  \bdd{x}_0 &= 0,\\
  \bdd{x}_j &= - \mathrm{G} \nobreak\hspace{.125em plus .14286em}
              m_0 \frac{\bp{x}_i}{|\bp{x}_i|^3}.
\end{align*}
In particular we can note that this system is stable.
Over time, it will never diverge far from its present state.

% TODO expand on the discussion of the solar system

\section{Hamiltonian Mechanics}
\label{sec:hamilt-mech}

We will need to consider Hamiltonian mechanics in our study of the KAM Theorem.
Hamiltonian mechanics is a reformulation of Newtonian mechanics.
It produces the same results, but it a more convenient formulation for some
problems.

A Hamiltonian mechanical system is given by an even-dimensional manifold, the
phase space, a symplectic structure on this manifold, and a function $H$.
This function is called a Hamiltonian function.

Let $(X, \sigma)$ be a symplectic manifold.
That is, let $X$ be a differentiable manifold and $\sigma$ be a nowhere
vanishing $2$-form such that $\dif \sigma = 0$.
A function $H$ on $X$ has a \emph{symplectic gradient}, denoted $\sgrad H$.
This gradient is defined as the unique vector field $\sgrad H$ such that for any
vector field $\xi$, we have
\begin{equation}
  \label{eq:symplectic-grad}
  \sigma(\xi, \sgrad H) = \dif H(\xi).
\end{equation}
We will be considering a \emph{Hamiltonian differential equation} that makes use
of this symplectic gradient:
\begin{equation}
  \label{eq:hamiltonian-diff-eq}
  \bd{x} = (\sgrad H) (\bp{x}).
\end{equation}

Before moving on, let's compare the symplectic gradient and the gradient $\nabla
f$ of a function $f : \R^n \to \R$.
This gradient is not an element of $\R$, but rather is the vector
\begin{equation*}
  \nabla f =
  \begin{bmatrix}
    \pd{f}{x_1}\\
    \vdots\\
    \pd{f}{x_n}
  \end{bmatrix}.
\end{equation*}
In the symplectic gradient, we used the symplectic form $\sigma$ to give a
geometric structure to our underlying space.
Similarly, we can define the gradient by using the scalar product $\langle
\cdot, \cdot \rangle$ to provide a geometric structure.
In comparison to \cref{eq:symplectic-grad}, the gradient $\nabla f$ is the
unique vector field such that
\begin{equation*}
  \dif f(\xi) = \langle \xi, \nabla f \rangle.
\end{equation*}
Further, just as we considered a Hamiltonian differential equation in
\cref{eq:hamiltonian-diff-eq}, we can consider a gradient differential equation:
\begin{equation*}
  \bd{x} = \nabla f(\bp{x}).
\end{equation*}
The gradient and Hamiltonian differential equations are fundamentally different.
In the gradient equation, our function $f$ increases along solutions.
Thus these solutions may never return to their starting points.
The gradient differential equation prohibits any type of recurrence from
occurring.
In contrast, the Hamiltonian differential equation not just allows but
practically imposes recurrent behavior.
This difference means we can observe much more interesting recurrent behavior
when using Hamilton's equation than we could with the gradient equation.

As an example of a Hamiltonian system, let's consider our toy model of the solar
system from \cref{sec:motivating-example}.
We take our manifold $X$ to be $\R^{2n}$ with coordinates $(q_1, \ldots, q_n,
p_1, \ldots, p_n)$.
Our symplectic form will be given by $\sigma = \sum_{i} \dif p_i \wedge \dif
q_i$.
The Hamiltonian differential equation then becomes the \emph{Hamiltonian
  equations of motion}:
% TODO could talk about the Math $4200$ two dimensional case of Hamiltonian vs
% gradient systems
\begin{equation}
  \label{eq:hamiltonian-motion-eq}
  \begin{aligned}
    \dot{q}_i &= \pd{H}{p_i},\\
    \dot{p}_i &= - \pd{H}{q_i}.
  \end{aligned}
\end{equation}

Now we consider the case of a single body of zero mass.
It is sufficient to consider the single body system, since the planets' having
zero mass means that they do not affect each other in our toy system.
For $\bp{x} \in \R^2$, the equation
\begin{equation*}
  \bdd{x} = - \frac{\bp{x}}{|\bp{x}|^3}
\end{equation*}
is Hamilton's equation for the manifold $X = \R^2 \times \R^2$ with points
$(\bp{q}, \bp{p})$, standard symplectic form
\begin{equation*}
  \sigma = \dif p_1 \wedge \dif q_1 + \dif p_2 \wedge \dif q_2,
\end{equation*}
and Hamiltonian
\begin{equation*}
  H(\bp{q}, \bp{p}) = \frac{1}{2} (p_1^2 + p_2^2) - \frac{1}{\sqrt{q_1^2 +
      q_2^2}}.
\end{equation*}
We write $\bp{p} = \pt{p_1\\ p_2}$ and $\bp{q} = \pt{q_1\\q_2}$.
Now we have
\begin{align*}
  (\dif p_1 \wedge \dif q_1 &+ \dif p_2 \wedge \dif q_2)
                              \left(\left(\pt{\xi_1\\\xi_2}, \pt{\eta_1\\\eta_2}
  \right), \left( \pt{p1\\p2}, - \frac{1}{(q_1^2 + q_2^2)^{\frac{3}{2}}}
  \pt{q1\\q2} \right)\right)\\
                            &= \eta_1 p_1 + \frac{\xi_1 q_1}{(q_1^2 +
                              q_2^2)^{\frac{3}{2}}} + \eta_2 p_2 + \frac{\xi_2
                              q_2}{(q_1^2 + q_2^2)^{\frac{3}{2}}}\\
                            &= \left[ \Dif H \left( \pt{q1\\q1}, \pt{p1\\p2}
  \right) \right] \left( \pt{\xi_1\\\xi_2}, \pt{\eta_1\\\eta_2} \right).
\end{align*}
Following from this computation, we see that
\begin{equation*}
  \sgrad H = \left(\bp{p}, - \frac{1}{(q_1^2 + q_2^2)^{\frac{3}{2}}}\bp{q}
  \right).
\end{equation*}
The Hamiltonian differential equation $\bp{x} = \sgrad H(\bp{x})$ is then given
by
\begin{align*}
  q_1' = p_1 &\qquad p_1' = - \frac{q_1}{(q_1^2 + q_2^2)^{\frac{3}{2}}},\\
  q_2' = p_2 &\qquad p_2' = -\frac{q_2}{(q_1^2 + q_2^2)^{\frac{3}{2}}}.
\end{align*}
From here, we can recover the desired $\bdd{x} = - \frac{\bp{x}}{|\bp{x}|^3}$.

Now let's again consider the Hamiltonian Differential Equation
\cref{eq:hamiltonian-diff-eq}.
The vector field $\sgrad f$ has a flow that we denote by $\phi_f^t$.
This flow has two properties:
\begin{itemize}
\item $\phi_f^t$ preserves $f$: $f \circ \phi_f^t = f$,
\item $\phi_f^t$ preserves $\sigma$: $(\phi_f^t)^* \sigma = \sigma$.
\end{itemize}
Flows will be central to our proof of Kolmogorov's Theorem.
We will be constructing a symplectic diffeomorphism and will need the flows of
Hamiltonian functions to do so.

Further, in our proof we will be constructing Taylor polynomials of functions of
the form $t \mapsto g \circ \phi_f^t$.
To perform this construction, we will use the Poisson bracket.
\begin{defn}[Poisson Bracket]
  Let $f$ and $g$ be two function on $X$.
  The Poisson bracket, denoted $\{f, g\}$, is defined by
  \begin{equation*}
    \{f, g\} = \sigma(\sgrad f, \sgrad g) = \dif f(\sgrad g) = - \dif g(\sgrad
 f).
  \end{equation*}
\end{defn}
We say that two functions $f$ and $g$ commute if the Poisson bracket is zero:
\begin{equation*}
  \{f, g\} = 0.
\end{equation*}
This implies that their flows also commute:
\begin{equation*}
  \phi_f(s) \circ \phi_g(t) = \phi_g(t) \circ \phi_f(s).
\end{equation*}
The Poisson bracket allows to write Taylor polynomials in the form
\begin{equation*}
  f \circ \phi_g^t = f + t\{f, g\} + \frac{t^2}{2} \{\{f, g\}, g\} +
  \frac{t^3}{3!} \{ \{ \{f, g\}, g\}, g\} + \cdots.
\end{equation*}

The Poisson bracket is related to the Lie bracket:
the Lie bracket is the symplectic gradient of the Poisson bracket.
\begin{prop}[Relation of Poisson and Lie Brackets]
  For any two functions $f$ and $g$ on a symplectic manifold $(X, \sigma)$, we
  have
  \begin{equation*}
    \sgrad \{f, g\} = [\sgrad f, \sgrad g].
  \end{equation*} 
\end{prop}
\begin{proof}
  This proof will use the Jacobi identity for the Poisson bracket:
  \begin{equation*}
    \{\{f, g\}, h\} + \{\{g, h\}, f\} + \{\{h, f\}, g\} = 0.
  \end{equation*}
  Given a function $h$, we can compute
  \begin{align*}
    \dif h([\sgrad f, \sgrad g]) &= \dif (\dif h (\sgrad h)) (\sgrad f) - \dif
                                   (\dif h (\sgrad f)) (\sgrad g)\\
                                 &= \dif \{h, g\} (\sgrad f) - \dif \{h, f\}
                       (\sgrad g)\\
                                 &= \{ \{h, g\}, f\} - \{ \{ h, f\}, g\}\\
                                 &= \{\{h, g\}, f\} + \{\{f, h\}, g\}\\
                                 &= \{\{f, g\}, h\}\\
                                 &= \dif h (\sgrad \{f, g\}).
  \end{align*}
\end{proof}
As an example of a Poisson bracket, let's consider again the Hamiltonian
equations of motion (\cref{eq:hamiltonian-motion-eq}).
The Poisson bracket is calculated by
\begin{equation*}
  \{f, g\} = \sum_{i=1}^n \left( \pd{f}{q_i} \pd{g}{p_i} - \pd{f}{p_i}
    \pd{g}{q_i} \right).
\end{equation*}

Next we discuss what it means for a system to be totally integrable.
We will be denoting the torus by $\T = \R / \Z$.
\begin{defn}[Totally Integrable System]
  A \emph{totally integrable system} is a symplectic manifold $X = \T^n \times
  \R^n$ with variables $(\bp{q} \in \T^n, \bp{p} \in \R^n)$, symplectic form
  $\sum_i \dif p_i \wedge \dif q_i$, and Hamiltonian function $H(\bp{p})$
  depending only on $\bp{p}$.
\end{defn}
The first example is to integrate the Hamiltonian equations of motion
(\cref{eq:hamiltonian-motion-eq}).
If we take $(\bp{q}_0, \bp{p}_0)$ to be the initial value and let $\omega =
\pd{H}{\bp{p}}$, then the solution is 
\begin{align*}
  \bp{q}(t) &= \bp{q}_0 + t \pd{H}{\bp{p}}(\bp{p}_0) = t \omega(\bp{p}_0),\\
  \bp{p}(t) &= \bp{p}_0.
\end{align*}
Note that in particular, each coordinate $p_1, \ldots p_n$ is conserved, and the
trajectory is a linear motion on the torus $\T^n \times \{\bp{p}_0\}$.

% TODO discuss more, perhaps using Louville's theorem

\section{Irrationality}
\label{sec:irrationality}

In our statement of Kolmogorov's Theorem, we included the hypothesis that
$\omega \in \Omega_{\gamma}$.
We now define this notation and begin to explain its importance.
The set $\Omega_{\gamma}$ consists of vectors that are ``sufficiently
irrational,'' a notion that we need to make more precise.

Let's first consider the definition of an irrational number.
If a real number $\theta$ is irrational, then for all pairs if integers $p$ and
$q$, with $q$ positive, we have the following
\begin{equation*}
  \left| \theta - \frac{p}{q} \right| \neq 0.
\end{equation*}
This equation tells us simply that there does not exist and rational number
$\frac{p}{q}$ that equals our irrational number $\theta$.
Here, the ``not equal to zero'' part of the equation will be stressed, as an
expression similar to the one on the left hand side will later appear as the
denominator of a fraction (see \cref{sec:dioph-diff-equat}).
As dividing by zero can be rather troublesome, we wish to avoid it.
This condition of irrationality is the tool we use to do so: if $\theta$ is
irrational, then the left side will not be zero, so we can divide by it without
any problems.

Our condition of ``sufficiently irrational'' will mean that $\left| \theta -
  \frac{p}{q} \right|$ is ``sufficiently nonzero,'' or since we are using an
absolute value, ``sufficiently big.''
However, as we learn in our introductory courses in real analysis, the rationals
are dense in the reals, and every real number, specifically every irrational
number $\theta$, may be approximated arbitrarily closely by the rationals.
More precisely, given any real $\epsilon > 0$, there exists a rational number
$\frac{p}{q}$ such that $\left| \theta - \frac{p}{q} \right| < \epsilon$.
Thus trying to coerce $\left| \theta - \frac{p}{q} \right|$ to be big is quite
impossible.

Unsatisfied with our answer, let's instead consider a different question.
Instead of wanting $\left| \theta - \frac{p}{q} \right|$ to be ``big,'', we ask
that it is small \emph{only if the denominator is big}.
This is the beginning of the theory of Diophantine approximation.

The numbers that we seek will satisfy the following definition.
\begin{defn}[Diophantine Number of Exponent $d$]
  \dionumber{}
\end{defn}
From this definition we see that it is a stronger requirement for a number to be
Diophantine of a smaller exponent.
For all irrational numbers $\theta$ there exist arbitrarily large $q$ and $p$
prime to $q$ such that
\begin{equation*}
  \left| \theta - \frac{p}{q} \right| < \frac{1}{\sqrt{5}q^2}.
\end{equation*}
We see that no number is Diophantine of any exponent smaller than $2$.
And the number that are Diophantine of exponent exactly $2$ are precisely the
numbers whose continued fractions have bounded entries.
These numbers form a set of measure zero.

But what about exponents greater than $2$, that is, of the form $2 + \epsilon$
for $\epsilon > 0$?
In the sense of Lebesgue measure, these numbers are quite abundant, as for any
$\epsilon > 0$ they form a set of full measure.
\begin{prop}[Diophantine Numbers with Full Measure]
  \label{prop:dio-num-with-full-meas}
  For all $\epsilon > 0$, the set of Diophantine numbers of exponent $2 +
  \epsilon$ is of full measure.
\end{prop}
\begin{proof}
  We consider numbers in $\R / \Z$.
  Given any positive integer $q$, there are at most $q$ elements of $\Q / \Z$
  that, in reduced form, have denominator $q$.
  Hence for any constant $\gamma$, we consider the set
  \begin{equation*}
    \left\{ \theta \in \R / \Z : \left| \theta - \frac{p}{q} \right| <
      \frac{\gamma}{|q|^{2 + \epsilon}} \right\}.
  \end{equation*}
  The length of this set is at most $\frac{2\gamma}{q^{1 + \epsilon}}$.
  Summing over all $q$, we see that the set of numbers $\theta$ for with there
  exists $q$ such that
  \begin{equation*}
    \left| \theta - \frac{p}{q} \right| < \frac{\gamma}{2^{2 + \epsilon}}
  \end{equation*}
  has length strictly less than
  \begin{equation*}
    2 \gamma \sum_{q = 1}^{\infty} \frac{1}{q^{1 + \epsilon}}.
  \end{equation*}
  Take the intersection over all these sets as $\gamma \to 0$ and note that this
  intersection has measure $0$.
  But this set is the complement of the set of Diophantine numbers of exponent
  $2 + \epsilon$.
  Hence the claim holds.
\end{proof}

Having this definition for numbers, we want to extend ``irrationality'' to
vectors.
As an example, we can consider the solar system with the vector $\omega =
(\omega_1, \ldots, \omega_n)$, where each $\omega_i$ represents the frequency of
the $i$th planet's orbit.
Our statement of Kolmogorov's theorem will require that such a vector be
irrational according to the following definition.
\begin{defn}[Diophantine Vector]
  \label{def:diovec}
  \diovector{}
\end{defn}
We could rewrite the condition in the definition as
\begin{equation}
  \label{eq:diovec}
  \bp{k} \cdot \omega \geq \frac{\gamma}{|\bp{k}|^n}.
\end{equation}
Also, we will often drop then $n$ from out notation and write simply
$\Omega_{\gamma}$ instead of $\Omega_{\gamma}^n$ when it is clear that we are
working in $\R^n$.

Again, we want to examine how common it is for such vectors to occur.
We do not want just exceptional motions to be preserved, but rather we wish that
most motions are preserved, and that we should not need to look hard to find
such vectors.
In the case of numbers, we have that satisfying answer that Diophantine numbers
have full measure.
We establish an analogous result for vectors, following a similar proof to the
one we have just seen.
\begin{prop}[Diophantine Vectors are of Full Measure]
  The union over $\gamma > 0$ of sets of Diophantine vectors with constant
  $\gamma$,
  \begin{equation*}
    \Omega = \bigcup_{\gamma > 0} \Omega_{\gamma},
  \end{equation*}
  is of full measure.
\end{prop}
\begin{proof}
  Consider the region $S_{\bp{k}, \gamma}$, in which
  \begin{equation*}
    |\bp{k} \cdot \omega| \leq \frac{\gamma}{|\bp{k}|^n}
  \end{equation*}
  is a region around the hyperplane orthogonal to $\bp{k}$ and with thickness
  $\frac{2\gamma}{|\bp{k}|^{n+1}}$.
  Denote the unit cube by $Q$.
  The part of $S_{\bp{k}, \gamma}$ within $Q$ has measure at most
  $\frac{\mathrm{M}\gamma}{|\bp{k}|^{n+1}}$, where $\mathrm{M}$ denotes the
  constant giving the maximal $(n-1)$-dimensional measure of the intersection of
  $Q$ with a hyperplane.
  Now consider the sum
  \begin{equation*}
    \sum_{k \in \Z^n \setminus \{0\}} \frac{1}{|\bp{k}|^{n+1}}.
  \end{equation*}
  This sum is finite, so the volume of
  \begin{equation*}
    \bigcup_{\mathclap{\bp{k} \in \Z^n \setminus \{0\}}} S_{\bp{k}, \gamma} \cap Q
  \end{equation*}
  is bounded by some constant times $\gamma$.
  As before, we now consider the intersection of these sets as $\gamma \to 0$:
  \begin{equation*}
     \bigcap_{\gamma > 0} \bigcup_{\bp{k} \in \Z^n \setminus \{0\}} S_{\bp{k},
       \gamma} \cap Q.
   \end{equation*}
   This intersection has measure $0$, and this set is the complement of our
   desired set $\Omega$.
   Thus $\Omega$ has full measure.

  Note that this proof is very similar to that of
  \cref{prop:dio-num-with-full-meas}, and reduces to the case $\epsilon = 1$ if
  we take $n = 2$.
\end{proof}

We can now understand the condition $\omega \in \Omega_{\gamma}$ from our
statement of Kolmogorov's Therorem.
This requirement means that the vector $\omega$ must be ``suitably irrational,''
and such vectors are rather ``common'' in the sense of Lebesgue.

% TODO remark about Biare meagerness

\section{KAM and the Solar System}
\label{sec:kam-solar-system}

With these preliminaries out of the way, we are going to discuss Kolmogorov's
Theorem in the context of our solar system example.

We view the solar as system as an $n$ dimensional torus, where $n$ is the number
of planets.
This gives us a geometric way to describe the motions of the planets.
Suppose that the planets have initial positions given by $\bp{a} = (a_1, \ldots,
a_n)$.
Then a trajectory with frequency vector $\omega = (\omega_1, \ldots, \omega_n)$
is at the point $\bp{a} + t \omega$ at time $t$.

Define a \emph{linear flow on $(\R/\Z)^n$ in the direction $\omega$} to be the
motion given by
\begin{equation*}
  t \mapsto \bp{a} + t \omega = (a_1 + t \omega_1, \ldots, a_n + t \omega_n).
\end{equation*}
Such a trajectory is dense on the torus if and only if $\omega$ is irrational
according to the Diophantine condition from \cref{def:diovec}.

Kolmogorov's Theorem tells us about when motions of a system are preserved, so
let's examine what it means for two motions to be the same as each other.
Consider a motion $\bp{x}(t)$ to be a motion of the perturbed system and a
motion $\bp{x_1}(t)$ to be a motion of the unperturbed system that is dense on a
torus $T_1$.
For the motions to be the same, we mean that $\bp{x}(t)$ is dense on the
corresponding torus $T$ and that it fills in $T$ in the same way that $\bp{x}_1$
fills in $T_1$.
% TODO explain the fill in combinatorially part
% Figure out what Matt had said
That is, there exists a homeomorphism $\Phi : T \to T_1$ such that
\begin{equation*}
  \Phi(\bp{x}(t)) = \bp{x}_1(t).
\end{equation*}

Using this notation, let's see what Kolmogorov's theorem says for our solar
system.
\begin{thm}[Kolmogorov's Theorem Applied to the Solar System]
  Let $\bp{x}_1(t)$ be a motion of the zero-masses system with Diophantine
  frequency vector.
  Then there exists $\epsilon > 0$ such that, if the planets are given masses
  $m_i < \epsilon$, there exists a trajectory $\bp{x}(t)$ of the perturbed
  system dense on a torus $T$ and a homeomorphism $\Phi : T \to T_1$ such that
  $\Phi(\bp{x}(t)) = \bp{x}_1(t)$.

  The set of such trajectories is of positive measure in the set of all
  trajectories.
  The probability of being on such a trajectory tends to $1$ as $\epsilon$ tends
  to zero.
\end{thm}

We would like that our solar system should fit the hypotheses of this theorem.
But alas, the periods of the orbits of Jupiter and Saturn at in a $5 : 2$ ratio.
Thus we have trouble with the important hypothesis of the irrationality of the
planets' frequency vector.
However, there are refinements of the theorem that state that there may still
exist stable motions where the ratios are rational.
We will not pursue these refinements here.
% TODO check up on these refinements

\section{Analytic Functions}
\label{sec:analytic-functions}

As this is a paper for a complex analysis class, we begin this section by noting
that the complex analysis is located here.

Take another look at the statement of the theorem.
We will concern ourselves with \cref{eq:sdiffeo}.
Here we have $H = h \circ \Phi$, where $\Phi$ is a symplectic diffeomorphism.
Our proof will involve solving for this diffeomorphism $\Phi$.
% TODO discuss the equations that we are solving
% Or maybe leave this to the main idea of the proof section

We need a tool for solving equations, and the tool that we use is similar to
Newton's Method.
So let's take a moment first to consider what goes on to using Newton's Method.

Newton's Method can be used to solve an equation $f(\bp{x}) = 0$.
The process involves choosing an initial guess $\bp{x}_0$.
Then successive points $\bp{x}_i$ are defined by
\begin{equation*}
  \bp{x}_{i+1} = \bp{x}_i - [Df(\bp{x}_i)]^{-1} f(\bp{x}_i).
\end{equation*}
These points $\bp{x}_{i+1}$ will, under ``good'' conditions, converge to a
solution to the equation.

To see why some additional conditions are necessary, consider the function $f(x)
= x^3 - x + \frac{\sqrt{2}}{2}$.
\begin{wrapfigure}{o}{0.5\textwidth}
  \begin{center}
    \includegraphics[width=0.48\textwidth]{newtonfail}
  \end{center}
  \caption{$f(x) = x^3 - x + \frac{\sqrt{2}}{2}$}
\end{wrapfigure}
Let our initial guess be $x_0 = 0$.
Note that this function has derivative $f'(x) = 3x^2 - 1$.
Thus plugging in our initial guess gives us
\begin{equation*}
  x_1 = x_0 - \frac{1}{f'(x_0)} f(x_0) = \frac{\sqrt{2}}{2}.
\end{equation*}
But then we encounter a problem once we use $x_1$ to determine $x_2$.
\begin{equation*}
  x_2 = x_1 - \frac{1}{f'(x_1)} f(x_1) = 0.
\end{equation*}
This brings us back to our initial guess!
So now Netwon's method will oscillate between $0$ and $\frac{\sqrt{2}}{2}$ and
will not converge to a solution.

The additional conditions that we need for the convergence of Newton's Method
are given in a theorem by Kantorovitch.
% TODO consider stating the theorem
In order to guarantee convergence, we need a bound on the second derivative of
the function $f$.

In solving for the diffeomorphism $\Phi$ in the proof of Kolmogorov's theorem,
we will be using a similar technique to Newton's method.
Where with Newton's method we needed a bound on the second derivative, we will
need some type of analogous condition for our iterative process.
We need some way to measure ``size,'' that is, we ned to choose a norm for our
functions.

First let's recall the definition of a Banach algebra.
\begin{defn}[Banach Algebra]
  Let $k$ be $\R$ or $\C$.
  A \emph{normed algebra} over $k$ is an algebra $\mathcal{A}$ over $k$ with a
  sub-multiplicative norm $\norm{\cdot}$.
  That is, for all $x, y \in \mathcal{A}$, we have
  \begin{equation*}
    \norm{xy} \leq \norm{x}\norm{y}.
  \end{equation*}
  If $\mathcal{A}$ is a Banach space, then it is called a Banach algebra.
\end{defn}
Let $X \subseteq \C^k$ be compact, and let the caligraphic letter $\mathcal{X}$
be the Banach algebra of continuous functions on $X$ that are analytic in the
interior and have the sup norm
\begin{equation*}
  \norm{f}_X = \sum_{\bp{x} \in X} |f(\bp{x})|.
\end{equation*}
Here we use the normal absolute value as the standard Euclidean norm on $\C^n$.


We consider three regions:
% TODO use these somewhere useful
\domains{}
Denote by $\mathcal{B}_{\rho}$, $\mathcal{C}_{\rho}$, and $\mathcal{A}_{\rho}$
the corresponding Banach algebras.
We can expand elements of $\mathcal{B}_{\rho}$ as power series, and elements of
$\mathcal{C}_{\rho}$ as Fourier series:
\begin{equation*}
  f(\bp{z}) = \sum_{\bp{k} \in \Z^n} f_{\bp{k}} \e^{2 \pi \i \bp{k} \cdot \bp{z}}.
\end{equation*}
This Fourier series expansion will be important in \cref{sec:dioph-diff-equat}.

Let's take a look at how we can bound derivatives of analytic functions,
analogous to how we use the second derivative to bound the derivative of
functions in Kantorovitch's Theorem.
We will use Cauchy's Inequalities on Balls.
\begin{thm}[Cauchy's Inequalities on Balls]
  If $f \in \mathcal{B}_{\rho}$, then
  \begin{align*}
    \norm{\Dif f}_{\rho - \delta} &\leq \frac{1}{\delta} \norm{f}_{\rho},\\
    \norm{\Dif^2 f}_{\rho - \delta} &\leq \frac{4}{\delta^2} \norm{f}_{\rho}.
  \end{align*}
  As a corollary, the case $\delta = \rho$ bounds the derivatives at the center
  of balls:
  \begin{align*}
    |\Dif f(0)| &\leq \frac{1}{\rho} \norm{f}_{\rho},\\
    |\Dif^2 f(0)| &\leq \frac{4}{\rho^4} \norm{f}_{\rho}.
  \end{align*}
\end{thm}
\begin{proof}
  % TODO expand on proof and include more details
  Take $\bp{z} \in B_{\rho - \delta}$ and $\bp{u} \in \C^n$.
  Since $B_{\delta}(\bp{z}) \subseteq B_{\rho}$, the function
  \begin{equation*}
    g : t \mapsto f(\bp{z} + t \delta \bp{u})
  \end{equation*}
  is defined on the unit disc.
  The normal Cauchy inequality implies that
  \begin{equation*}
    \delta|(\Dif f(\bp{z})) \bp{u}| = |g'(0)| \leq \norm{g}_{1} \leq
    \norm{f}_{\rho}.
  \end{equation*}
  Applying the argument twice gives the result for the second derivative:
  \begin{equation*}
    |\Dif^2 f(\bp{z})(\bp{u}, \bp{v})| \leq \frac{2}{\delta} \norm{\Dif f
      (\bp{z})(\bp{u})}_{\rho - \frac{\delta}{2}}|\bp{v}| \leq
    \frac{4}{\delta^2} \norm{f}_{\rho} |\bp{u}||\bp{v}|.
  \end{equation*}
\end{proof}

\section{Main Idea of the Proof}
\label{sec:main-idea-proof}

Let's take another look at the statement of Kolmogorov's theorem and recall
\cref{eq:sdiffeo}, where we have $H = h \circ \Phi$:
\begin{equation*}
  H(\bp{Q}, \bp{P}) = A + \omega \bp{P} + R(\bp{Q}, \bp{P}).
\end{equation*}
This equation is an equation for a diffeomorphism $\Phi$, and the proof involves
solving for this diffeomorphism.
We would like to use Newton's method to do this solving, but that in itself is
not quite sufficient for out purposes.
However, we can still do something with a similar flavor.

The proof uses an iterative process in which we obtain the diffeomorphism $\Phi$
as a limit of $\Phi_i$, where
\begin{equation*}
  \Phi_i = \phi_i \circ \phi_{i - 1} \circ \cdots \circ \phi_1.
\end{equation*}
Here, $\phi_i$ denotes the Hamiltonian flow $\phi_{g_i}$ for a Hamiltonian
function $g_i$.
This $g_i$ is the unknown for which we solve.

% TODO pause and take a moment to reflect on how we went from a diffeomorphism
% to a function.
Let's take a moment to pause and reflect on what was just done.
Instead of solving for a diffeomorphism $\Phi$, at each iteration we are solving
for a Hamiltonian function $g_i$.
This simplification is rather important, as a diffeomorphism can be a rather
complicated object.
The $g_i$ are much easier to reason about.
Furthermore, the map $\phi_{g_i}$ is itself a symplectic diffeomorphism.
% TODO continue reflection

At the $i$th iteration we have a Hamiltonian $\tilde{h} = \Phi_i^* h$.
We expand $\tilde{h}$ up to order $2$ in $\bp{p}$, and the coefficients are
Fourier series in $\bp{q}$.
We write $\tilde{h} = \tilde{h}_0 + \tilde{h}_1$, where
\begin{itemize}
\item $\tilde{h}_1$ has the terms constant or linear in $\bp{p}$, except the
  constant in $\bp{q}$,
\item $\tilde{h}_0$ is everything else.
\end{itemize}
We would like to eliminate $\tilde{h}_1$, be we won't be able to do so by
solving a linear equation.
Instead we will solve a linear equation for a function $g$ such that
$\phi_g^*\tilde{h}$ is ``better'' than $\tilde{h}$, in some reasonable sense of
``better.''

We expand $\phi_g^* \tilde{h}$ to first order in $g$.
This gives us
\begin{align*}
  \phi_g^* \tilde{h} &= \tilde{h} + \{g, \tilde{h}\} + o(|g|)\\
                     &= \tilde{h}_0 + \tilde{h}_1 + \{g, \tilde{h}_0\} + \{g,
                       \tilde{h}_1\} + o(|g|).
\end{align*}
We want to eliminate the terms, other than the term constant in $\bp{q}$, that
are not $O(|\bp{p}|)^2$.
Applying the standard Newton's method would necessitate solving the equation
\begin{equation*}
  \tilde{h}_1 + \{g, \tilde{h}_0\} + \{g, \tilde{h}_1\} \in o(|\bp{p}|).
\end{equation*}
But we will do something a bit different.

We can assume that anything we want is small as long as the choice is justified
by the resulting inequalities.
So suppose that $\{g, \tilde{h}_1\}$ is of order $2$ since $g$ and $\tilde{h}_1$
are both small.
Then the linear equation that we need to solve is
\begin{equation*}
  \tilde{h}_1 + \{g, \tilde{h}_0\} \in o(|\bp{p}|).
\end{equation*}
Thus we need to solve this \emph{Diophantine differential equation}.
Our ability to solve it relies on the Diophantine vectors that we saw previously
in \cref{sec:irrationality}.

\section{Diophantine Differential Equations}
\label{sec:dioph-diff-equat}

Let $g \in \mathcal{C}_{\rho}$.
We are going to be solving linear equations of the form
\begin{equation*}
  \Dif f(\omega) = \sum_{i = 1}^n \omega_i \pd{f}{q_i} = g,
\end{equation*}
with $f \in \mathcal{C}_{\rho'}$ for some $\rho' < \rho$.
Recall, as was mentioned in \cref{sec:analytic-functions}, that since $f \in
\mathcal{C}_{\rho'}$ and $g \in \mathcal{C}_{\rho}$, we can write $f$ and $g$ as
Fourier Series.
\begin{align*}
  f(\bp{q}) &= \sum_{\bp{k} \in \Z^n} f_{\bp{k}} \e^{2 \pi \i \bp{k} \cdot
    \bp{q}},\\
  g(\bp{q}) &= \sum_{\bp{k} \in \Z^n} g_{\bp{k}} \e^{2 \pi \i \bp{k} \cdot \bp{q}}.
\end{align*}
The solution is given by
\begin{equation}
  \label{eq:dio-sol}
  f_{\bp{k}} = \frac{1}{2 \pi \i (\bp{k} \cdot \omega)}g_{\bp{k}}.
\end{equation}
We need $g_0$ to be zero.
Then $f_0$ is arbitrary, and otherwise the series for $f$ is unique.

Take note of the $(\bp{k} \cdot \omega)$ in the denominator of \cref{eq:dio-sol}
and compare this to \cref{eq:diovec}.
Dividing by zero in \cref{eq:dio-sol} would mean that there is no solution.
So we need the Diophantine condition for the vector $\omega$ to ensure that we
do not divide by zero in \cref{eq:dio-sol}.
Hence the convergence properties of $f$ depend on the Diophantine properties of
$\omega$.

But even though this condition gives us convergence for $\rho$, we might not
have boundedness.
So we need some $\rho' < \rho$ where we have boundedess.
But we must choose $\rho'$ large enough so that the limit is nonempty.
Thus the choice of $\rho'$ must be done rather ``carefully.''

We will use the following tool for choosing $\rho'$.
% TODO name proposition
\begin{prop}
  If $g \in \mathcal{C}_{\rho}$ and $\epsilon \in \Omega_{\gamma}$, then for all
  $\delta$ such that $0 < \delta < \rho$, we have the following two
  inequalities:
  \begin{align*}
    \norm{f}_{\rho - \delta} &\leq \frac{\kappa_n}{\gamma
                               \delta^{2n}}\norm{g}_{\rho},\\
    \norm{\Dif f}_{\rho - \delta} &\leq \frac{\kappa_n}{\gamma \delta^{2n + 1}}
                    \norm{g}_{\rho}.
  \end{align*}
  Here, $\kappa_n$ is a constant that depends only on $n$.
\end{prop}
\begin{proof}
  For $\bp{y} \in \R^n$ with $|\bp{y} \leq \rho$ (in particular $\bp{y} = \rho
  \frac{\bp{k}}{|\bp{k}|})$, the function
  \begin{equation*}
    \bp{q} \mapsto g(\bp{q} - \i \bp{y})
  \end{equation*}
  is continuous and periodic in $\bp{q}$ of period $1$.
  This function can be written
  \begin{equation*}
    g(\bp{q} - \i \bp{y}) = \sum_{\bp{k} \in \Z^n} g_{\bp{k}} \e^{2 \pi \i
      \bp{k} \cdot (\bp{q} - \i \bp{y})} = \sum_{\bp{k} \in \Z^n}
    \left(g_{\bp{k}} \e^{2 \pi \bp{k} \cdot \bp{y}} \right) \e^{2 \pi \i \bp{k}
      \cdot \bp{q}}.
  \end{equation*}
  By Parseval's theorem, we have
  \begin{equation*}
    \norm{g}_{\rho}^2 \geq \int_{\T^n} |g(\bp{q} - \i \bp{y})|^2 |\dif ^n
    \bp{q}| = \sum_{\bp{k} \in \Z^n} |g_{\bp{k}}|^2 \e^{4 \pi \bp{k} \cdot \bp{y}}.
  \end{equation*}
  Since the series has positive numbers, we have
  \begin{equation*}
    \norm{g}_{\rho}^2 \geq |g_{\bp{k}}|^2 \e^{4 \pi \rho|\bp{k}|}.
  \end{equation*}

  Using the Diophantine-ness of $\omega$ and the Fourier coefficients
  $f_{\bp{k}}$, we have
  \begin{equation*}
    |f_{\bp{k}}| \leq \frac{1}{2 \pi \gamma} \norm{g}_{\rho} |\bp{k}|^n \e^{-2
      \pi |\bp{k}| \rho}.
  \end{equation*}

  Next we split the proposition into two parts: the first part dealing with the
  $f$ case and the second part dealing with the $\Dif f$ case.

  First we handle the case for $f$.
  For $|\bp{q}| \leq \rho - \delta$, we can write
  \begin{align*}
    \left| f_{\bp{k}} \e^{2 \pi \i (\bp{k} \cdot \bp{q})} \right|
    &\leq \sum_{\bp{k} \in \Z^n} \frac{|\bp{k}|^n}{2\pi\gamma} \norm{g}_{\rho}
      \e^{-2 \pi \rho |\bp{k}|(\rho - \delta)}\\
    &= \frac{\norm{g}_{\rho}}{2 \pi \gamma} \sum_{\bp{k} \in \Z^n} |\bp{k}|^n
      \e^{-2 \pi |\bp{k}|\delta}.
  \end{align*}
  We then rewrite the sum on the right:
  \begin{equation*}
    \sum_{\bp{k} \in \Z^n} |\bp{k}|^n \e^{-2 \pi |\bp{k}|\delta} = \frac{1}{(2
      \pi \delta)^{2n}} \left((2 \pi \delta)^n \sum_{\bp{k}' \in 2 \pi \delta
        \Z^n} |\bp{k}'|^n \e^{-|\bp{k}'|} \right).
  \end{equation*}
  As $\delta \to 0$, the expression inside the parentheses approaches
  \begin{equation*}
    \int_{\R^n} |\bp{x}|^n \e^{-|\bp{x}|} |\dif^n \bp{x}|.
  \end{equation*}
  Since the integral is convergent there exist $\kappa_n'$ such that for $\delta
  \leq 1$, it holds that
  \begin{equation*}
    \sum_{\bp{k} \in \Z^n} |\bp{k}|^n \e^{-2 \i |\bp{k}|\delta} \leq
    \frac{\kappa_n'}{(2 \pi \delta)^{2n}}.
  \end{equation*}
  Thus we are finished with the $f$ part of the proof.

  The $\Dif f$ part of the proof is similar.
  We use
  \begin{equation*}
    \Dif f(\bp{q})(\bp{u}) = 2 \pi \i \sum(\bp{k} \cdot \bp{u}) f_{\bp{k}} \e^{2
      \pi \i (\bp{k} \cdot \bp{q})}.
  \end{equation*}
  Then we obtain the inequality
  \begin{equation*}
    |\Dif(\bp{q})(\bp{u})| \leq \frac{|\bp{u}| \norm{g}_{\rho}
      \kappa_n''}{\gamma(2 \pi \gamma)^{n+1}}.
  \end{equation*}

  To complete the proof of the proposition, take
  \begin{equation*}
    \kappa_n := \max \left\{\frac{\kappa_n'}{(2n)^{2n}},
      \frac{\kappa_n''}{(2n)^{2n+1}}\right\}.
  \end{equation*}
\end{proof}

\section{Solving Equations}
\label{sec:solving-equations}

Let's return to our previous equation
\begin{equation*}
  h_1 + \{g, h_0\} \in O(|\bp{p}|^2).
\end{equation*}
% TODO reference first equation
This equation is the Diophantine differential equation that we had before, but
without tildes and with $o(|\bp{p}|)$ replaced by $O(|\bp{p}|^2)$, which is
equivalent since our functions are analytic.
The unknown is $g$, which we take to be degree $1$ in $\bp{p}$:
\begin{equation*}
  g = \lambda \bp{q} X(\bp{q}) + \sum_{i=1}^n Y_i(\bp{q}) p_i.
\end{equation*}
Note that only the linear terms of $g$ contribute to the linear terms of $\{g,
h_0\}$.
The unknowns are $\lambda$, $X$, and $Y_i$.
We can expand $X$ and $Y_i$ as Fourier series, since they are functions of
$\bp{q}$ only.

Let $a \in \R$, $\omega \in \Omega_{\gamma}$, $R(\bp{q}, \bp{p}) \in
O(|\bp{p}|^3)$, and $\overline{A} = 0$, where $\overline{A}$ is the average of
$A$ on the torus $\bp{p} = 0$.
We can then write
\begin{align*}
  h_0(\bp{q}, \bp{p}) &= a + \omega \bp{p} + \frac{1}{2} \bp{p} \cdot C(\bp{q})
                        \bp{p} + R(\bp{q}, \bp{p}),\\
  h_1(\bp{q}, \bp{p}) &= A(\bp{q}) + B(\bp{q}) \bp{p}.
\end{align*}
We then have
\begin{equation*}
  (h_1 + \{g, h_0\}) (\bp{q}, \bp{p}) = \omega \cdot \lambda + A(\bp{q}) + \Dif
  X(\bp{q}) (\omega) + \Big( B(\bp{q}) + \big(\lambda + \Dif X(\bp{q}) \big)
  C(\bp{q}) + \omega \Dif Y(\bp{q}) \Big) \cdot \bp{p} + O(|\bp{p}|^2).
\end{equation*}

We need to solve the equations
\begin{align*}
  \Dif X(\bp{q}) (\omega) &= - A(\bp{q})\\
  \Dif Y(\bp{q}) (\omega) &= -B(\bp{q}) - \big(\lambda + \Dif
     X(\bp{q})\big)C(\bp{q})
\end{align*}
for $X$ and $Y_i$, $i = 1, \ldots, n$.
These are Diophantine differential equations.
Since we have the hypothesis $\overline{A} = 0$, we can solve the first equation
and find $X \in \mathcal{C}_{\rho'}$ for all $\rho' < \rho$.
We then substitute into the second equation.
We can then determine $\lambda$ since the averages of the right-hand sides in
the $n$ are all zero.
Then we can colve the second equation and find $Y_i \in \mathcal{C}_{\rho''}$
for all $\rho'' < \rho'$.
Iterating the process requires choosing $\rho'$ and $\rho''$ ``carefully.''


\section{Conclusion}
\label{sec:conclusion}

With everything that we have discussed, the statement of Kolmogorov's Theorem
should now be very much readable.
We have an equation that we solve for a diffeomorphism.
This equation is solved iteratively using an analogue of Newton's Method.
We used analytic functions in order for this technique to converge properly,
analogous to how we would need to bound the second derivative is we were to
apply Kantorovitch's Theorem when using Newton's Method.
And we need the Diophantine vectors in order to avoid dividing by zero when
solving for our Fourier coefficients.
% TODO what have we missed

The proof by Hubbard and Ilyshenko in \cite{hi02} is quite short, and solely by
following it without considering the work before it we may miss out on what make
it so succinct, and why the simplifications that it makes are so powerful and
useful.
% TODO cite Arnold's original proof and many other proofs of the theorem

% TODO add works to bibliography and update references throughout paper
% TODO add Lee's Smooth Manifolds as a reference
\bibliographystyle{plain}
\bibliography{bibliography}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
